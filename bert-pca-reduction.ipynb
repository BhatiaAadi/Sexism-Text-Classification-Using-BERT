{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10996967,"sourceType":"datasetVersion","datasetId":6845513}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install transformers datasets torch scikit-learn pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T22:27:43.407723Z","iopub.execute_input":"2025-03-16T22:27:43.407950Z","iopub.status.idle":"2025-03-16T22:27:48.173176Z","shell.execute_reply.started":"2025-03-16T22:27:43.407929Z","shell.execute_reply":"2025-03-16T22:27:48.172114Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"As in the previous notebook we tried to reduce overfitting through- regularisation and k-fold cross validation \nHere in this code we try the dimensionality reduction technique to remove overfitting.\nwe particularly use PCA in here because this has proved to be better than svd because as it saves variance in the reduced dimensions","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/sexism-classification-dataset-csv/sexism_classification_dataset.csv\")\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ndf[\"label_id\"] = label_encoder.fit_transform(df[\"label_vector\"])\n\n# Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df[\"text\"].tolist(), df[\"label_id\"].tolist(), test_size=0.2, random_state=42\n)\n\n# Load tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# Create Dataloaders\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Load BERT model for fine-tuning\nnum_labels = len(label_encoder.classes_)\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Fine-tune BERT\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n\ndef train(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for batch in dataloader:\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef extract_bert_embeddings(model, dataloader, device):\n    model.eval()\n    embeddings, labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_mask, label = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"]\n            outputs = model.bert(input_ids, attention_mask=attention_mask)\n            hidden_states = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Mean pooling\n            embeddings.extend(hidden_states)\n            labels.extend(label.numpy())\n    return embeddings, labels\n\n# Train BERT\nepochs = 3\nfor epoch in range(epochs):\n    train_loss = train(model, train_loader, optimizer, criterion, device)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")\n\n# Extract BERT embeddings for train and val sets\ntrain_embeddings, train_labels = extract_bert_embeddings(model, train_loader, device)\nval_embeddings, val_labels = extract_bert_embeddings(model, val_loader, device)\n\n# Apply PCA\nscaler = StandardScaler()\npca = PCA(n_components=300)  # Dimensionality reduction\ntrain_embeddings = pca.fit_transform(scaler.fit_transform(train_embeddings))\nval_embeddings = pca.transform(scaler.transform(val_embeddings))\n\n# Define simple neural classifier\nclass NeuralClassifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(NeuralClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.softmax(self.fc3(x))\n        return x\n\n# Convert numpy arrays to tensors\ntrain_embeddings = torch.tensor(train_embeddings, dtype=torch.float32)\nval_embeddings = torch.tensor(val_embeddings, dtype=torch.float32)\ntrain_labels = torch.tensor(train_labels, dtype=torch.long)\nval_labels = torch.tensor(val_labels, dtype=torch.long)\n\n# Create DataLoader for Neural Classifier\ntrain_data = torch.utils.data.TensorDataset(train_embeddings, train_labels)\nval_data = torch.utils.data.TensorDataset(val_embeddings, val_labels)\n\ntrain_loader = DataLoader(train_data, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n\n# Train the Neural Classifier\ninput_dim = 300  # Same as PCA output\ndnn_model = NeuralClassifier(input_dim, num_labels).to(device)\noptimizer = optim.Adam(dnn_model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ndef train_nn(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for batch in dataloader:\n        optimizer.zero_grad()\n        inputs, labels = batch[0].to(device), batch[1].to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef evaluate_nn(model, dataloader, device):\n    model.eval()\n    predictions, true_labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n            outputs = model(inputs)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels.cpu().numpy())\n    return accuracy_score(true_labels, predictions)\n\n# Train DNN Classifier\nfor epoch in range(5):\n    train_loss = train_nn(dnn_model, train_loader, optimizer, criterion, device)\n    val_acc = evaluate_nn(dnn_model, val_loader, device)\n    print(f\"Epoch {epoch+1}/5, Loss: {train_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T20:46:51.735442Z","iopub.execute_input":"2025-03-16T20:46:51.735848Z","iopub.status.idle":"2025-03-16T20:51:52.516795Z","shell.execute_reply.started":"2025-03-16T20:46:51.735819Z","shell.execute_reply":"2025-03-16T20:51:52.515306Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 1.8520\nEpoch 2/3, Loss: 1.3665\nEpoch 3/3, Loss: 0.9752\nEpoch 1/5, Loss: 1.7889, Val Accuracy: 0.5294\nEpoch 2/5, Loss: 1.7030, Val Accuracy: 0.5273\nEpoch 3/5, Loss: 1.6893, Val Accuracy: 0.5242\nEpoch 4/5, Loss: 1.6742, Val Accuracy: 0.5221\nEpoch 5/5, Loss: 1.6664, Val Accuracy: 0.5129\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"I did not think that 300 dimensions were capturing enough information so what I did was use the variance component of PCA \nformat in which we set the dimensions to include 98% variance \nLets see if that help us in getting better accuracy","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/sexism-classification-dataset-csv/sexism_classification_dataset.csv\")\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ndf[\"label_id\"] = label_encoder.fit_transform(df[\"label_vector\"])\n\n# Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df[\"text\"].tolist(), df[\"label_id\"].tolist(), test_size=0.2, random_state=42\n)\n\n# Load tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# Load BERT model\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_model.to(device)\n\n# Extract BERT embeddings\ndef extract_embeddings(texts, tokenizer, model, device):\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n        for text in texts:\n            encoded = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n            input_ids = encoded['input_ids'].to(device)\n            attention_mask = encoded['attention_mask'].to(device)\n            output = model(input_ids, attention_mask=attention_mask)\n            embeddings.append(output.last_hidden_state[:, 0, :].cpu().numpy())\n    return torch.tensor(embeddings).squeeze()\n\ntrain_embeddings = extract_embeddings(train_texts, tokenizer, bert_model, device)\nval_embeddings = extract_embeddings(val_texts, tokenizer, bert_model, device)\n\n# Apply PCA to retain 98% variance\npca = PCA(n_components=0.98)\ntrain_pca = pca.fit_transform(train_embeddings)\nval_pca = pca.transform(val_embeddings)\n\n# Convert to torch tensors\ntrain_pca = torch.tensor(train_pca, dtype=torch.float32)\nval_pca = torch.tensor(val_pca, dtype=torch.float32)\ntrain_labels = torch.tensor(train_labels, dtype=torch.long)\nval_labels = torch.tensor(val_labels, dtype=torch.long)\n\n# Custom Dataset for NN class\nclass EmbeddingDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.embeddings = embeddings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.embeddings[idx], self.labels[idx]\n\ntrain_dataset = EmbeddingDataset(train_pca, train_labels)\nval_dataset = EmbeddingDataset(val_pca, val_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Define deeper NN model\nclass DeepNeuralNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(DeepNeuralNet, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n    \n    def forward(self, x):\n        x = self.relu(self.bn1(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.relu(self.bn2(self.fc2(x)))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n# Model setup\ninput_dim = train_pca.shape[1]\nnum_classes = len(label_encoder.classes_)\nmodel = DeepNeuralNet(input_dim, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training function\ndef train_model(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\n# Evaluation function\ndef evaluate_model(model, dataloader, device):\n    model.eval()\n    predictions, true_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels.cpu().numpy())\n    return accuracy_score(true_labels, predictions)\n\n# Training loop\nepochs = 10\nfor epoch in range(epochs):\n    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n    val_acc = evaluate_model(model, val_loader, device)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T21:13:55.146126Z","iopub.execute_input":"2025-03-16T21:13:55.146454Z","iopub.status.idle":"2025-03-16T21:14:51.621045Z","shell.execute_reply.started":"2025-03-16T21:13:55.146430Z","shell.execute_reply":"2025-03-16T21:14:51.620213Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-28199d0fad22>:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.tensor(embeddings).squeeze()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 2.2170, Val Accuracy: 0.3223\nEpoch 2/10, Loss: 1.8979, Val Accuracy: 0.3357\nEpoch 3/10, Loss: 1.7745, Val Accuracy: 0.3574\nEpoch 4/10, Loss: 1.6664, Val Accuracy: 0.3656\nEpoch 5/10, Loss: 1.5936, Val Accuracy: 0.3656\nEpoch 6/10, Loss: 1.5240, Val Accuracy: 0.3800\nEpoch 7/10, Loss: 1.4583, Val Accuracy: 0.3893\nEpoch 8/10, Loss: 1.3869, Val Accuracy: 0.3852\nEpoch 9/10, Loss: 1.3235, Val Accuracy: 0.4006\nEpoch 10/10, Loss: 1.2642, Val Accuracy: 0.3986\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"But this certainly not helped and we saw a very dramatic change in the accuracy\nSo what do we do now? So I tried to implement a Roberta Model instead of Bert to get better accuracy as:\n1) It is trained on 10 times larger datset than BERT\n2) When the roberta model was trained in that, the researchers used masking techniques at training time that means whenever the sentence is taken for training we mask random words which helps it in generalising for not only next word prediction tasks but others as well\n3) On the otherhand bert is trained in such a way that it is efficient for only next word prediction because of non-dynamic masking. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}