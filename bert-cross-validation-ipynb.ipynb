{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10996967,"sourceType":"datasetVersion","datasetId":6845513}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/sexism-classification-dataset-csv/sexism_classification_dataset.csv\")\n\n# Encode labels as integers\nlabel_encoder = LabelEncoder()\ndf[\"label_id\"] = label_encoder.fit_transform(df[\"label_vector\"])\n\n# Load tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# Training function\ndef train(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    predictions, true_labels = [], []\n    \n    for batch in dataloader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        \n        # Get predictions\n        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n        predictions.extend(preds)\n        true_labels.extend(labels.cpu().numpy())\n        \n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    # Calculate accuracy\n    train_accuracy = accuracy_score(true_labels, predictions)\n    return total_loss / len(dataloader), train_accuracy\n\n# Evaluation function\ndef evaluate(model, dataloader, device):\n    model.eval()\n    predictions, true_labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels.cpu().numpy())\n    return accuracy_score(true_labels, predictions)\n\n# K-Fold Cross Validation Implementation\ndef k_fold_cross_validation(df, num_folds=5, epochs=5, batch_size=16, learning_rate=1e-5, weight_decay=0.01):\n    # Prepare data\n    texts = df[\"text\"].tolist()\n    labels = df[\"label_id\"].tolist()\n    \n    # Initialize k-fold\n    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n    \n    # Device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Track metrics across folds\n    fold_train_losses = []\n    fold_train_accs = []\n    fold_val_accs = []\n    \n    # For early stopping\n    best_val_acc = 0\n    best_model_state = None\n    \n    # Run K-fold cross validation\n    for fold, (train_idx, val_idx) in enumerate(kf.split(texts)):\n        print(f\"\\nFold {fold+1}/{num_folds}\")\n        \n        # Split data\n        fold_train_texts = [texts[i] for i in train_idx]\n        fold_train_labels = [labels[i] for i in train_idx]\n        fold_val_texts = [texts[i] for i in val_idx]\n        fold_val_labels = [labels[i] for i in val_idx]\n        \n        # Create datasets and dataloaders\n        train_dataset = TextDataset(fold_train_texts, fold_train_labels, tokenizer)\n        val_dataset = TextDataset(fold_val_texts, fold_val_labels, tokenizer)\n        \n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n        \n        # Initialize model for this fold\n        num_labels = len(label_encoder.classes_)\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", \n            num_labels=num_labels,\n            hidden_dropout_prob=0.3,\n            attention_probs_dropout_prob=0.3\n        )\n        model.to(device)\n        \n        # Optimizer\n        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n        criterion = torch.nn.CrossEntropyLoss()\n        \n        # Training loop for this fold\n        fold_best_val_acc = 0\n        patience = 3\n        no_improve_count = 0\n        \n        for epoch in range(epochs):\n            train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n            val_acc = evaluate(model, val_loader, device)\n            \n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n            \n            # Check for improvement\n            if val_acc > fold_best_val_acc:\n                fold_best_val_acc = val_acc\n                no_improve_count = 0\n                \n                # Track best model overall\n                if val_acc > best_val_acc:\n                    best_val_acc = val_acc\n                    best_model_state = model.state_dict().copy()\n            else:\n                no_improve_count += 1\n            \n            # Early stopping check\n            if no_improve_count >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n        \n        # Record metrics for this fold\n        fold_train_losses.append(train_loss)\n        fold_train_accs.append(train_acc)\n        fold_val_accs.append(fold_best_val_acc)\n    \n    # Print cross-validation results\n    print(\"\\nCross-Validation Results:\")\n    print(f\"Average Train Loss: {np.mean(fold_train_losses):.4f}\")\n    print(f\"Average Train Accuracy: {np.mean(fold_train_accs):.4f}\")\n    print(f\"Average Validation Accuracy: {np.mean(fold_val_accs):.4f}\")\n    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n    \n    # Load best model\n    final_model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", \n        num_labels=num_labels,\n        hidden_dropout_prob=0.3,\n        attention_probs_dropout_prob=0.3\n    )\n    final_model.load_state_dict(best_model_state)\n    final_model.to(device)\n    \n    return final_model, best_val_acc\n\n# Run k-fold cross validation\nnum_folds = 5  # You can adjust this\nepochs_per_fold = 5  # Fewer epochs per fold to manage computational resources\nbatch_size = 16\nlearning_rate = 5e-6  # Lower learning rate\nweight_decay = 0.01\n\nbest_model, best_acc = k_fold_cross_validation(\n    df,\n    num_folds=num_folds,\n    epochs=epochs_per_fold,\n    batch_size=batch_size,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay\n)\n\nprint(f\"\\nTraining complete! Best validation accuracy: {best_acc:.4f}\")\n\n# If you want to save the best model\nmodel_save_path = \"best_bert_model.pt\"\ntorch.save(best_model.state_dict(), model_save_path)\nprint(f\"Best model saved to {model_save_path}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:19:23.701001Z","iopub.execute_input":"2025-03-19T15:19:23.701346Z","iopub.status.idle":"2025-03-19T15:58:41.552880Z","shell.execute_reply.started":"2025-03-19T15:19:23.701313Z","shell.execute_reply":"2025-03-19T15:58:41.552042Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e75d4220414c18ab9be886b1a3a9a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5765bfdfdcb427da076e5a0fe60b319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b533b476ae544039aec9bf794c54e4ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33958c8cf1a44f1281d4a906bbbf05ea"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n\nFold 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1de1668cbfd43e5b179a9de03d34234"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 2.2038, Train Acc: 0.2037, Val Acc: 0.2987\nEpoch 2/5, Loss: 1.9841, Train Acc: 0.3013, Val Acc: 0.3296\nEpoch 3/5, Loss: 1.8723, Train Acc: 0.3309, Val Acc: 0.3841\nEpoch 4/5, Loss: 1.7702, Train Acc: 0.3701, Val Acc: 0.4099\nEpoch 5/5, Loss: 1.6892, Train Acc: 0.3966, Val Acc: 0.4408\n\nFold 2/5\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 2.1953, Train Acc: 0.2024, Val Acc: 0.2626\nEpoch 2/5, Loss: 1.9865, Train Acc: 0.2895, Val Acc: 0.3234\nEpoch 3/5, Loss: 1.8406, Train Acc: 0.3407, Val Acc: 0.3811\nEpoch 4/5, Loss: 1.7149, Train Acc: 0.3804, Val Acc: 0.3893\nEpoch 5/5, Loss: 1.6185, Train Acc: 0.4182, Val Acc: 0.4089\n\nFold 3/5\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 2.2057, Train Acc: 0.2063, Val Acc: 0.2719\nEpoch 2/5, Loss: 2.0312, Train Acc: 0.2704, Val Acc: 0.3223\nEpoch 3/5, Loss: 1.8897, Train Acc: 0.3240, Val Acc: 0.3821\nEpoch 4/5, Loss: 1.7675, Train Acc: 0.3685, Val Acc: 0.4222\nEpoch 5/5, Loss: 1.6654, Train Acc: 0.4133, Val Acc: 0.4501\n\nFold 4/5\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 2.2025, Train Acc: 0.1926, Val Acc: 0.2667\nEpoch 4/5, Loss: 1.7636, Train Acc: 0.3546, Val Acc: 0.3563\nEpoch 5/5, Loss: 1.6563, Train Acc: 0.3953, Val Acc: 0.4295\n\nFold 5/5\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 2.1856, Train Acc: 0.2070, Val Acc: 0.2309\nEpoch 2/5, Loss: 2.0089, Train Acc: 0.2752, Val Acc: 0.2979\nEpoch 3/5, Loss: 1.8796, Train Acc: 0.3296, Val Acc: 0.3165\nEpoch 4/5, Loss: 1.7624, Train Acc: 0.3610, Val Acc: 0.3557\nEpoch 5/5, Loss: 1.6622, Train Acc: 0.4032, Val Acc: 0.4144\n\nCross-Validation Results:\nAverage Train Loss: 1.6583\nAverage Train Accuracy: 0.4053\nAverage Validation Accuracy: 0.4287\nBest Validation Accuracy: 0.4501\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nTraining complete! Best validation accuracy: 0.4501\nBest model saved to best_bert_model.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"In this we tried cross-validation but that also did not work in improving the accuracy so we go to dimensionality reduction techniques as they might reduce this overfitting and then we would get a better accuracy\nSo we implemented pca reduction technique and then train a ffnn based architecture on that\n","metadata":{}}]}