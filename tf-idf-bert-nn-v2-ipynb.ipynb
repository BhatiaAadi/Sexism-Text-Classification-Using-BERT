{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10996967,"sourceType":"datasetVersion","datasetId":6845513}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# this code works but does it really work?\n!pip install transformers datasets torch scikit-learn pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-18T18:00:59.266833Z","iopub.execute_input":"2025-03-18T18:00:59.267053Z","iopub.status.idle":"2025-03-18T18:01:03.999715Z","shell.execute_reply.started":"2025-03-18T18:00:59.267029Z","shell.execute_reply":"2025-03-18T18:01:03.998910Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/sexism-classification-dataset-csv/sexism_classification_dataset.csv\")\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ndf[\"label_id\"] = label_encoder.fit_transform(df[\"label_vector\"])\n\n# Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df[\"text\"].tolist(), df[\"label_id\"].tolist(), test_size=0.2, random_state=42\n)\n\n# Load tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Create TF-IDF vectors with more features\nprint(\"Creating TF-IDF vectors...\")\ntfidf_vectorizer = TfidfVectorizer(\n    max_features=3000,  # Increased from 1000 to 3000\n    min_df=2,           # Minimum document frequency\n    ngram_range=(1, 2)  # Include both unigrams and bigrams\n)\ntfidf_vectorizer.fit(train_texts)\ntrain_tfidf = tfidf_vectorizer.transform(train_texts).toarray()\nval_tfidf = tfidf_vectorizer.transform(val_texts).toarray()\n\nprint(f\"Original TF-IDF features shape: {train_tfidf.shape}\")\n\n# Apply PCA to reduce dimensionality while preserving 95% variance\nprint(\"Applying PCA...\")\npca = PCA(n_components=0.95)  # Keep 95% of variance\npca.fit(train_tfidf)\ntrain_tfidf_pca = pca.transform(train_tfidf)\nval_tfidf_pca = pca.transform(val_tfidf)\n\nprint(f\"After PCA - features reduced to {train_tfidf_pca.shape[1]} components\")\nprint(f\"Variance explained: {sum(pca.explained_variance_ratio_):.4f}\")\n\n# Dataset class for BERT + TF-IDF-PCA\nclass CombinedDataset(Dataset):\n    def __init__(self, texts, tfidf_vectors, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.tfidf_vectors = tfidf_vectors\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"tfidf_vector\": torch.tensor(self.tfidf_vectors[idx], dtype=torch.float),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# Create dataset objects\ntrain_dataset = CombinedDataset(train_texts, train_tfidf_pca, train_labels, tokenizer)\nval_dataset = CombinedDataset(val_texts, val_tfidf_pca, val_labels, tokenizer)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Define the combined model\nclass CombinedModel(nn.Module):\n    def __init__(self, bert_model, tfidf_dim, hidden_dim, num_classes):\n        super(CombinedModel, self).__init__()\n        self.bert = bert_model\n        self.dropout = nn.Dropout(0.1)\n        \n        # Combined dimensions: BERT (768) + TF-IDF-PCA dimensions\n        combined_dim = 768 + tfidf_dim\n        \n        # Feed-forward layers with an additional hidden layer for better representation\n        self.classifier = nn.Sequential(\n            nn.Linear(combined_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim // 2, num_classes)\n        )\n        \n    def forward(self, input_ids, attention_mask, tfidf_vector):\n        # Get BERT embeddings (last hidden state)\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        # Use the [CLS] token representation\n        bert_embeddings = bert_output.last_hidden_state[:, 0, :]\n        \n        # Concatenate BERT embeddings with TF-IDF-PCA vector\n        combined_features = torch.cat((bert_embeddings, tfidf_vector), dim=1)\n        combined_features = self.dropout(combined_features)\n        \n        # Pass through classifier\n        logits = self.classifier(combined_features)\n        return logits\n\n# Initialize BERT model\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Initialize the combined model\ntfidf_dim = train_tfidf_pca.shape[1]  # Dimension of PCA-reduced TF-IDF vectors\nhidden_dim = 512\nnum_labels = len(label_encoder.classes_)\nmodel = CombinedModel(bert_model, tfidf_dim, hidden_dim, num_labels)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nmodel.to(device)\n\n# Set up optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Phase 1: Fine-tune BERT first\ndef train_bert_only(model, dataloader, optimizer, criterion, device, bert_only=True):\n    model.train()\n    total_loss = 0\n    \n    for batch in dataloader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        tfidf_vector = batch[\"tfidf_vector\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        # If bert_only is True, only update BERT parameters\n        if bert_only:\n            # Freeze the non-BERT parameters\n            for name, param in model.named_parameters():\n                if \"bert\" not in name:\n                    param.requires_grad = False\n        else:\n            # Unfreeze all parameters\n            for param in model.parameters():\n                param.requires_grad = True\n        \n        # Forward pass\n        logits = model(input_ids, attention_mask, tfidf_vector)\n        loss = criterion(logits, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\n# Phase 2: Train the combined model with full parameters\ndef train_combined(model, dataloader, optimizer, criterion, device):\n    return train_bert_only(model, dataloader, optimizer, criterion, device, bert_only=False)\n\n# Evaluation function\ndef evaluate(model, dataloader, device):\n    model.eval()\n    predictions, true_labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            tfidf_vector = batch[\"tfidf_vector\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            logits = model(input_ids, attention_mask, tfidf_vector)\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels.cpu().numpy())\n    \n    acc = accuracy_score(true_labels, predictions)\n    macro_f1 = f1_score(true_labels, predictions, average='macro')\n    weighted_f1 = f1_score(true_labels, predictions, average='weighted')\n    \n    return {\n        'accuracy': acc,\n        'macro_f1': macro_f1,\n        'weighted_f1': weighted_f1,\n        'predictions': predictions,\n        'true_labels': true_labels\n    }\n\n# For comparison, implement BERT-only model evaluation\ndef evaluate_bert_only(texts, labels, device):\n    # Create a standard BERT model\n    bert_only_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n    bert_only_model.to(device)\n    \n    # Create dataset and dataloader\n    class TextDataset(Dataset):\n        def __init__(self, texts, labels, tokenizer, max_len=128):\n            self.texts = texts\n            self.labels = labels\n            self.tokenizer = tokenizer\n            self.max_len = max_len\n\n        def __len__(self):\n            return len(self.texts)\n\n        def __getitem__(self, idx):\n            encoding = self.tokenizer(\n                self.texts[idx],\n                truncation=True,\n                padding='max_length',\n                max_length=self.max_len,\n                return_tensors='pt'\n            )\n            return {\n                \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n                \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n                \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n            }\n    \n    bert_dataset = TextDataset(texts, labels, tokenizer)\n    bert_loader = DataLoader(bert_dataset, batch_size=16, shuffle=True)\n    \n    # Optimizer\n    bert_optimizer = AdamW(bert_only_model.parameters(), lr=2e-5)\n    \n    # Training function for BERT-only\n    def train_bert(model, dataloader, optimizer, device):\n        model.train()\n        total_loss = 0\n        for batch in dataloader:\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            \n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        return total_loss / len(dataloader)\n    \n    # Evaluation function for BERT-only\n    def eval_bert(model, dataloader, device):\n        model.eval()\n        predictions, true_labels = [], []\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                \n                outputs = model(input_ids, attention_mask=attention_mask)\n                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n                \n                predictions.extend(preds)\n                true_labels.extend(labels.cpu().numpy())\n        \n        acc = accuracy_score(true_labels, predictions)\n        macro_f1 = f1_score(true_labels, predictions, average='macro')\n        weighted_f1 = f1_score(true_labels, predictions, average='weighted')\n        \n        return {\n            'accuracy': acc,\n            'macro_f1': macro_f1,\n            'weighted_f1': weighted_f1\n        }\n    \n    # Train BERT-only for 3 epochs\n    print(\"\\nTraining BERT-only model for comparison...\")\n    for epoch in range(3):\n        train_loss = train_bert(bert_only_model, bert_loader, bert_optimizer, device)\n        eval_metrics = eval_bert(bert_only_model, bert_loader, device)\n        print(f\"Epoch {epoch+1}/3, Loss: {train_loss:.4f}, Accuracy: {eval_metrics['accuracy']:.4f}, Macro F1: {eval_metrics['macro_f1']:.4f}\")\n    \n    # Final evaluation\n    final_metrics = eval_bert(bert_only_model, bert_loader, device)\n    return final_metrics\n\n# Training loop - Phase 1: Fine-tune BERT\nprint(\"Phase 1: Fine-tuning BERT...\")\nbert_epochs = 3\nfor epoch in range(bert_epochs):\n    train_loss = train_bert_only(model, train_loader, optimizer, criterion, device)\n    val_metrics = evaluate(model, val_loader, device)\n    print(f\"Epoch {epoch+1}/{bert_epochs}, Loss: {train_loss:.4f}, Val Accuracy: {val_metrics['accuracy']:.4f}, Macro F1: {val_metrics['macro_f1']:.4f}\")\n\n# Training loop - Phase 2: Train the combined model with increased epochs\nprint(\"\\nPhase 2: Training the combined model...\")\ncombined_epochs = 5  # Increased from 3 to 5\nfor epoch in range(combined_epochs):\n    train_loss = train_combined(model, train_loader, optimizer, criterion, device)\n    val_metrics = evaluate(model, val_loader, device)\n    print(f\"Epoch {epoch+1}/{combined_epochs}, Loss: {train_loss:.4f}, Val Accuracy: {val_metrics['accuracy']:.4f}, Macro F1: {val_metrics['macro_f1']:.4f}\")\n\n# Final evaluation on validation data\nprint(\"\\nFinal Evaluation of Combined Model on Validation Data:\")\nfinal_metrics = evaluate(model, val_loader, device)\nprint(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\nprint(f\"Macro F1 Score: {final_metrics['macro_f1']:.4f}\")\nprint(f\"Weighted F1 Score: {final_metrics['weighted_f1']:.4f}\")\n\n# Print detailed classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(\n    final_metrics['true_labels'], \n    final_metrics['predictions'], \n    target_names=label_encoder.classes_\n))\n\n# Compare with BERT-only approach\n# print(\"\\nComparing with BERT-only approach:\")\n# bert_only_metrics = evaluate_bert_only(train_texts, train_labels, device)\n# print(\"\\nBERT-only Final Metrics:\")\n# print(f\"Accuracy: {bert_only_metrics['accuracy']:.4f}\")\n# print(f\"Macro F1 Score: {bert_only_metrics['macro_f1']:.4f}\")\n# print(f\"Weighted F1 Score: {bert_only_metrics['weighted_f1']:.4f}\")\n\nprint(\"\\nCombined Model Final Metrics:\")\nprint(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\nprint(f\"Macro F1 Score: {final_metrics['macro_f1']:.4f}\")\nprint(f\"Weighted F1 Score: {final_metrics['weighted_f1']:.4f}\")\n\nif final_metrics['accuracy'] > bert_only_metrics['accuracy']:\n    improvement = ((final_metrics['accuracy'] - bert_only_metrics['accuracy']) / bert_only_metrics['accuracy']) * 100\n    print(f\"\\nCombined model improves accuracy by {improvement:.2f}%\")\nelse:\n    decrease = ((bert_only_metrics['accuracy'] - final_metrics['accuracy']) / bert_only_metrics['accuracy']) * 100\n    print(f\"\\nCombined model decreases accuracy by {decrease:.2f}%\")\n\n# Feature importance analysis\nprint(\"\\nFeature Importance Analysis:\")\nprint(f\"BERT embedding dimension: 768\")\nprint(f\"TF-IDF PCA dimensions: {tfidf_dim}\")\nprint(f\"Total feature dimensions: {768 + tfidf_dim}\")\nprint(f\"Top 10 PCA components explain {sum(pca.explained_variance_ratio_[:10])*100:.2f}% of variance\")\n\n# Save the model and preprocessing components\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'tfidf_vectorizer': tfidf_vectorizer,\n    'pca': pca,\n    'label_encoder': label_encoder\n}, \"combined_bert_tfidf_pca_model.pt\")\nprint(\"\\nModel and preprocessing components saved as 'combined_bert_tfidf_pca_model.pt'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T18:01:57.432946Z","iopub.execute_input":"2025-03-18T18:01:57.433285Z","iopub.status.idle":"2025-03-18T18:15:02.310302Z","shell.execute_reply.started":"2025-03-18T18:01:57.433261Z","shell.execute_reply":"2025-03-18T18:15:02.309089Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae5751d18fe446bb9629fe550713b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706d8ed6f25540c59ffe06749036045b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"339a7bb9f42a495b89fb7f467eddca69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c9b1ff69a954b3eaf7e6aefab0f6f6d"}},"metadata":{}},{"name":"stdout","text":"Creating TF-IDF vectors...\nOriginal TF-IDF features shape: (3883, 3000)\nApplying PCA...\nAfter PCA - features reduced to 1780 components\nVariance explained: 0.9501\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd2778df52b44bb0a9cd5cdf2acf631e"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Phase 1: Fine-tuning BERT...\nEpoch 1/3, Loss: 2.2536, Val Accuracy: 0.3903, Macro F1: 0.1336\nEpoch 2/3, Loss: 2.1265, Val Accuracy: 0.4408, Macro F1: 0.1898\nEpoch 3/3, Loss: 2.0589, Val Accuracy: 0.4552, Macro F1: 0.2058\n\nPhase 2: Training the combined model...\nEpoch 1/5, Loss: 1.5614, Val Accuracy: 0.4809, Macro F1: 0.2507\nEpoch 2/5, Loss: 1.0858, Val Accuracy: 0.5180, Macro F1: 0.3283\nEpoch 3/5, Loss: 0.8069, Val Accuracy: 0.5314, Macro F1: 0.3430\nEpoch 4/5, Loss: 0.5574, Val Accuracy: 0.5046, Macro F1: 0.3638\nEpoch 5/5, Loss: 0.3881, Val Accuracy: 0.5160, Macro F1: 0.3727\n\nFinal Evaluation of Combined Model on Validation Data:\nAccuracy: 0.5160\nMacro F1 Score: 0.3727\nWeighted F1 Score: 0.5116\n\nClassification Report:\n                          precision    recall  f1-score   support\n\n       Broad Gender Bias       0.51      0.54      0.52        72\n   Dismissive Addressing       0.00      0.00      0.00        16\n     Everyday Derogation       0.63      0.56      0.59       190\nFixed Gender Perceptions       0.49      0.61      0.54       118\n     Harmful Provocation       0.60      0.61      0.60        69\n          Hostile Speech       0.49      0.52      0.51       186\n    Masked Disparagement       0.00      0.00      0.00        13\n         Menacing Speech       0.23      0.21      0.22        14\n    Singular Gender Bias       0.16      0.19      0.17        21\n    Stripping Personhood       0.35      0.42      0.38        60\n      Verbal Degradation       0.56      0.53      0.55       212\n\n                accuracy                           0.52       971\n               macro avg       0.37      0.38      0.37       971\n            weighted avg       0.51      0.52      0.51       971\n\n\nCombined Model Final Metrics:\nAccuracy: 0.5160\nMacro F1 Score: 0.3727\nWeighted F1 Score: 0.5116\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-859f20d0dd31>\u001b[0m in \u001b[0;36m<cell line: 342>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Weighted F1 Score: {final_metrics['weighted_f1']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mfinal_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbert_only_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0mimprovement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbert_only_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbert_only_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nCombined model improves accuracy by {improvement:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'bert_only_metrics' is not defined"],"ename":"NameError","evalue":"name 'bert_only_metrics' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"Here we did not compare our model with Bert_only metrics so that is the reason of error I have commented out the code for that.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}