{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10996967,"sourceType":"datasetVersion","datasetId":6845513}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook we are implementing Roberta model:-","metadata":{}},{"cell_type":"code","source":"# this code works but does it really work?\n!pip install transformers datasets torch scikit-learn pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T21:16:10.987957Z","iopub.execute_input":"2025-03-11T21:16:10.988250Z","iopub.status.idle":"2025-03-11T21:16:15.541386Z","shell.execute_reply.started":"2025-03-11T21:16:10.988221Z","shell.execute_reply":"2025-03-11T21:16:15.540505Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/sexism-classification-dataset-csv/sexism_classification_dataset.csv\")\n\n# Encode labels as integers\nlabel_encoder = LabelEncoder()\ndf[\"label_id\"] = label_encoder.fit_transform(df[\"label_vector\"])\n\n# Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df[\"text\"].tolist(), df[\"label_id\"].tolist(), test_size=0.2, random_state=42\n)\n\n# Load RoBERTa tokenizer instead of BERT\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# Create Dataloaders\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Load RoBERTa model instead of BERT\nnum_labels = len(label_encoder.classes_)\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nmodel.to(device)\n\n# Optimizer and Loss\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Training function\ndef train(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    predictions, true_labels = [], []\n    \n    for batch in dataloader:\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        \n        # Get predictions for training accuracy\n        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n        predictions.extend(preds)\n        true_labels.extend(labels.cpu().numpy())\n        \n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    # Calculate training accuracy\n    train_accuracy = accuracy_score(true_labels, predictions)\n    return total_loss / len(dataloader), train_accuracy\n\n# Evaluation function\ndef evaluate(model, dataloader, device):\n    model.eval()\n    predictions, true_labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels.cpu().numpy())\n    return accuracy_score(true_labels, predictions)\n\n# Training loop\nepochs = 10\nfor epoch in range(epochs):\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n    val_acc = evaluate(model, val_loader, device)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Val Accuracy: {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T21:23:43.305546Z","iopub.execute_input":"2025-03-11T21:23:43.305949Z","iopub.status.idle":"2025-03-11T21:32:30.675834Z","shell.execute_reply.started":"2025-03-11T21:23:43.305921Z","shell.execute_reply":"2025-03-11T21:32:30.674942Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 1.7562, Train Accuracy: 0.3549, Val Accuracy: 0.4892\nEpoch 2/10, Loss: 1.3002, Train Accuracy: 0.5297, Val Accuracy: 0.5304\nEpoch 3/10, Loss: 1.0166, Train Accuracy: 0.6443, Val Accuracy: 0.5427\nEpoch 4/10, Loss: 0.7273, Train Accuracy: 0.7517, Val Accuracy: 0.5273\nEpoch 5/10, Loss: 0.5124, Train Accuracy: 0.8357, Val Accuracy: 0.4943\nEpoch 6/10, Loss: 0.3340, Train Accuracy: 0.8996, Val Accuracy: 0.5304\nEpoch 7/10, Loss: 0.2501, Train Accuracy: 0.9227, Val Accuracy: 0.5335\nEpoch 8/10, Loss: 0.1521, Train Accuracy: 0.9598, Val Accuracy: 0.5273\nEpoch 9/10, Loss: 0.1302, Train Accuracy: 0.9652, Val Accuracy: 0.5118\nEpoch 10/10, Loss: 0.1182, Train Accuracy: 0.9642, Val Accuracy: 0.5314\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Here we faced the same issue as our own bert model that is of overfitting problem but here we don't go over the same process as we did before \nNow we try to understand the dataset that as we have used different techniques but that does not improve accuracy\nSo we use create new embeddings based on the dataset and we try to improve its accuracy by combining both the bert embeddings and new embeddings \nwnow use new embedings to concatenate with bert and hence we'll have the features of both our embeddings and then we train dnn models to get our classification task done\nI am first training a tf-idf based embeddings then concatanating to ouyr bert and then run a ffnn based architecture on those embeddings ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}