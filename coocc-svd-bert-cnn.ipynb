{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-03-18T13:29:34.991453Z","iopub.status.busy":"2025-03-18T13:29:34.991154Z","iopub.status.idle":"2025-03-18T13:29:40.649488Z","shell.execute_reply":"2025-03-18T13:29:40.648423Z","shell.execute_reply.started":"2025-03-18T13:29:34.991423Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"]}],"source":["# this code works but does it really work?\n","!pip install transformers datasets torch scikit-learn pandas"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-03-18T13:29:57.526927Z","iopub.status.busy":"2025-03-18T13:29:57.526590Z","iopub.status.idle":"2025-03-18T13:37:41.610509Z","shell.execute_reply":"2025-03-18T13:37:41.609584Z","shell.execute_reply.started":"2025-03-18T13:29:57.526897Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6969768b72854eef92cdaf474d672e3f","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41650bd0020f40428e6f669c56a527cf","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dedffc0d480c4ea5bf7e778bf6354611","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0dc6e7d9009d4f57809f22834544c4fc","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c402fc1d44b4d38a229b143d3653466","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Step 1: Fine-tuning BERT for 3 epochs\n","BERT Fine-tuning - Epoch 1/3, Loss: 1.8581, Val Accuracy: 0.4459\n","BERT Fine-tuning - Epoch 2/3, Loss: 1.3827, Val Accuracy: 0.5366\n","BERT Fine-tuning - Epoch 3/3, Loss: 0.9850, Val Accuracy: 0.5355\n","Step 2: Creating SVD embeddings with dimension 300\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Step 3: Training the CNN-based combined model\n","Combined Model - Epoch 1/3, Loss: 1.6608, Val Accuracy: 0.5232\n","Combined Model - Epoch 2/3, Loss: 0.7394, Val Accuracy: 0.5458\n","Combined Model - Epoch 3/3, Loss: 0.6102, Val Accuracy: 0.5427\n","\n","Step 4: Evaluating the combined model on the test set\n","Test Accuracy: 0.5427\n","\n","Classification Report:\n","                          precision    recall  f1-score   support\n","\n","       Broad Gender Bias       0.63      0.43      0.51        72\n","   Dismissive Addressing       0.00      0.00      0.00        16\n","     Everyday Derogation       0.68      0.58      0.63       190\n","Fixed Gender Perceptions       0.52      0.56      0.54       118\n","     Harmful Provocation       0.67      0.65      0.66        69\n","          Hostile Speech       0.49      0.63      0.55       186\n","    Masked Disparagement       0.25      0.08      0.12        13\n","         Menacing Speech       0.25      0.07      0.11        14\n","    Singular Gender Bias       0.24      0.19      0.21        21\n","    Stripping Personhood       0.40      0.38      0.39        60\n","      Verbal Degradation       0.53      0.60      0.56       212\n","\n","                accuracy                           0.54       971\n","               macro avg       0.42      0.38      0.39       971\n","            weighted avg       0.54      0.54      0.53       971\n","\n","Test results saved to 'test_results.csv'\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from scipy.sparse import csr_matrix\n","\n","# Load dataset\n","df = pd.read_csv(\"/kaggle/input/sexism-classification-dataset-csv/sexism_classification_dataset.csv\")\n","\n","# Encode labels as integers\n","label_encoder = LabelEncoder()\n","df[\"label_id\"] = label_encoder.fit_transform(df[\"label_vector\"])\n","\n","# Train-test split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    df[\"text\"].tolist(), df[\"label_id\"].tolist(), test_size=0.2, random_state=42\n",")\n","\n","# Load tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","# PART 1: FINE-TUNE BERT FIRST (as in original code)\n","# Dataset class for BERT fine-tuning\n","class BertDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","    \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, idx):\n","        encoding = self.tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_len,\n","            return_tensors='pt'\n","        )\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","# Create datasets and dataloaders for BERT fine-tuning\n","bert_train_dataset = BertDataset(train_texts, train_labels, tokenizer)\n","bert_val_dataset = BertDataset(val_texts, val_labels, tokenizer)\n","bert_train_loader = DataLoader(bert_train_dataset, batch_size=16, shuffle=True)\n","bert_val_loader = DataLoader(bert_val_dataset, batch_size=16, shuffle=False)\n","\n","# Load BERT model for fine-tuning\n","num_labels = len(label_encoder.classes_)\n","bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n","\n","# Use GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","bert_model.to(device)\n","\n","# Optimizer and Loss\n","optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training function for BERT\n","def train_bert(model, dataloader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","        input_ids, attention_mask, labels = (\n","            batch[\"input_ids\"].to(device),\n","            batch[\"attention_mask\"].to(device),\n","            batch[\"labels\"].to(device)\n","        )\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","# Evaluation function for BERT\n","def evaluate_bert(model, dataloader, device):\n","    model.eval()\n","    predictions, true_labels = [], []\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids, attention_mask, labels = (\n","                batch[\"input_ids\"].to(device),\n","                batch[\"attention_mask\"].to(device),\n","                batch[\"labels\"].to(device)\n","            )\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    return accuracy_score(true_labels, predictions), predictions, true_labels\n","\n","# BERT fine-tuning loop\n","print(\"Step 1: Fine-tuning BERT for 3 epochs\")\n","bert_epochs = 3\n","for epoch in range(bert_epochs):\n","    train_loss = train_bert(bert_model, bert_train_loader, optimizer, criterion, device)\n","    val_acc, _, _ = evaluate_bert(bert_model, bert_val_loader, device)\n","    print(f\"BERT Fine-tuning - Epoch {epoch+1}/{bert_epochs}, Loss: {train_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n","\n","# Save the fine-tuned BERT model\n","torch.save(bert_model.state_dict(), \"fine_tuned_bert.pt\")\n","\n","# PART 2: CREATE SVD EMBEDDINGS\n","# Create co-occurrence matrix and SVD embeddings\n","def create_svd_embeddings(texts, svd_dim=300):\n","    # Create co-occurrence matrix using CountVectorizer\n","    vectorizer = CountVectorizer(max_features=10000)\n","    count_matrix = vectorizer.fit_transform(texts)\n","    \n","    # Apply SVD for dimensionality reduction\n","    svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n","    svd_embeddings = svd.fit_transform(count_matrix)\n","    \n","    return vectorizer, svd, svd_embeddings\n","\n","# Create SVD embeddings with higher dimensions (300)\n","svd_dim = 300\n","print(f\"Step 2: Creating SVD embeddings with dimension {svd_dim}\")\n","vectorizer, svd_model, train_svd_embeddings = create_svd_embeddings(train_texts, svd_dim)\n","val_count_matrix = vectorizer.transform(val_texts)\n","val_svd_embeddings = svd_model.transform(val_count_matrix)\n","\n","# PART 3: COMBINED MODEL WITH CNN\n","# Extract pre-trained BERT model without classification head\n","bert_base = BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","# Copy weights from fine-tuned BERT to the base model\n","bert_base.load_state_dict({k.replace('bert.', ''): v for k, v in bert_model.state_dict().items() \n","                          if 'bert.' in k}, strict=False)\n","\n","# Dataset class for combined model\n","class CombinedDataset(Dataset):\n","    def __init__(self, texts, labels, svd_embeddings, tokenizer, max_len=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.svd_embeddings = svd_embeddings\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","    \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, idx):\n","        encoding = self.tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_len,\n","            return_tensors='pt'\n","        )\n","        \n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n","            \"svd_embeddings\": torch.tensor(self.svd_embeddings[idx], dtype=torch.float),\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","# CNN model that applies convolutions after concatenating BERT and SVD embeddings\n","class BertSvdCnnModel(nn.Module):\n","    def __init__(self, bert_model, num_labels, svd_dim=300):\n","        super(BertSvdCnnModel, self).__init__()\n","        self.bert = bert_model\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # Freeze BERT parameters to use pre-fine-tuned weights\n","        for param in self.bert.parameters():\n","            param.requires_grad = False\n","        \n","        # Dimensions\n","        self.bert_dim = 768\n","        self.svd_dim = svd_dim\n","        self.combined_dim = self.bert_dim + self.svd_dim\n","        \n","        # CNN layers for classification\n","        # 1D convolutions with different kernel sizes\n","        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv1d(256, 128, kernel_size=3, padding=1)\n","        \n","        # Pooling\n","        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n","        \n","        # Calculate the output size after convolutions and pooling\n","        # For combined_dim of 1068 (768 + 300):\n","        # After conv1: 1068 -> 1068 (same padding)\n","        # After pool: 1068 -> 534\n","        # After conv2: 534 -> 534 (same padding)\n","        # After pool: 534 -> 267\n","        # After conv3: 267 -> 267 (same padding)\n","        # After pool: 267 -> 133\n","        final_cnn_output_size = 128 * 133\n","        \n","        # Linear layers\n","        self.fc1 = nn.Linear(final_cnn_output_size, 256)\n","        self.fc2 = nn.Linear(256, num_labels)\n","        \n","    def forward(self, input_ids, attention_mask, svd_embeddings):\n","        # Get BERT embeddings for the CLS token\n","        with torch.no_grad():  # Don't compute gradients for BERT\n","            bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","            bert_cls_emb = bert_outputs.last_hidden_state[:, 0, :]  # [batch_size, bert_dim]\n","        \n","        # Concatenate BERT CLS embedding with SVD embeddings\n","        combined_emb = torch.cat((bert_cls_emb, svd_embeddings), dim=1)  # [batch_size, combined_dim]\n","        \n","        # Reshape for CNN (batch_size, channels, sequence_length)\n","        combined_emb = combined_emb.unsqueeze(1)  # [batch_size, 1, combined_dim]\n","        \n","        # Apply CNN layers\n","        x = self.pool(torch.relu(self.conv1(combined_emb)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = self.pool(torch.relu(self.conv3(x)))\n","        \n","        # Flatten\n","        x = x.view(x.size(0), -1)\n","        \n","        # Apply linear layers\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        logits = self.fc2(x)\n","        \n","        return logits\n","\n","# Create datasets and dataloaders for combined model\n","combined_train_dataset = CombinedDataset(train_texts, train_labels, train_svd_embeddings, tokenizer)\n","combined_val_dataset = CombinedDataset(val_texts, val_labels, val_svd_embeddings, tokenizer)\n","combined_train_loader = DataLoader(combined_train_dataset, batch_size=16, shuffle=True)\n","combined_val_loader = DataLoader(combined_val_dataset, batch_size=16, shuffle=False)\n","\n","# Initialize the combined model\n","combined_model = BertSvdCnnModel(bert_base, num_labels, svd_dim=svd_dim)\n","combined_model.to(device)\n","\n","# Optimizer for combined model\n","combined_optimizer = AdamW(combined_model.parameters(), lr=2e-5)\n","combined_criterion = nn.CrossEntropyLoss()\n","\n","# Training function for combined model\n","def train_combined(model, dataloader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","        \n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        svd_embeddings = batch[\"svd_embeddings\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        \n","        logits = model(input_ids, attention_mask, svd_embeddings)\n","        loss = criterion(logits, labels)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","        \n","    return total_loss / len(dataloader)\n","\n","# Evaluation function for combined model\n","def evaluate_combined(model, dataloader, device):\n","    model.eval()\n","    predictions, true_labels = [], []\n","    \n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            svd_embeddings = batch[\"svd_embeddings\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            \n","            logits = model(input_ids, attention_mask, svd_embeddings)\n","            preds = torch.argmax(logits, dim=1).cpu().numpy()\n","            \n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","            \n","    return accuracy_score(true_labels, predictions), predictions, true_labels\n","\n","# Training loop for combined model\n","print(\"Step 3: Training the CNN-based combined model\")\n","combined_epochs = 3\n","for epoch in range(combined_epochs):\n","    train_loss = train_combined(combined_model, combined_train_loader, combined_optimizer, combined_criterion, device)\n","    val_acc, _, _ = evaluate_combined(combined_model, combined_val_loader, device)\n","    print(f\"Combined Model - Epoch {epoch+1}/{combined_epochs}, Loss: {train_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n","\n","# Save the combined model\n","torch.save(combined_model.state_dict(), \"bert_svd_cnn_model.pt\")\n","\n","# Evaluate the combined model on the test set and generate detailed metrics\n","print(\"\\nStep 4: Evaluating the combined model on the test set\")\n","test_acc, test_predictions, test_true_labels = evaluate_combined(combined_model, combined_val_loader, device)\n","print(f\"Test Accuracy: {test_acc:.4f}\")\n","\n","# Print detailed classification report\n","class_names = label_encoder.classes_\n","print(\"\\nClassification Report:\")\n","print(classification_report(test_true_labels, test_predictions, target_names=class_names))\n","\n","# Convert predictions to original labels\n","test_pred_labels = label_encoder.inverse_transform(test_predictions)\n","test_true_orig_labels = label_encoder.inverse_transform(test_true_labels)\n","\n","# Create a dataframe with test results\n","test_results = pd.DataFrame({\n","    'Text': [val_texts[i] for i in range(len(val_texts))],\n","    'True Label': test_true_orig_labels,\n","    'Predicted Label': test_pred_labels,\n","    'Correct': test_true_orig_labels == test_pred_labels\n","})\n","\n","# Save test results to CSV\n","test_results.to_csv('test_results.csv', index=False)\n","print(\"Test results saved to 'test_results.csv'\")"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["This lead to a good increase in the accuracy but we have to still go a long way"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":6845513,"sourceId":10996967,"sourceType":"datasetVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
